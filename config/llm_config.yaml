# LLM Configuration for OPS Strategic Director

# Provider: openai, anthropic, or local (rule-based)
provider: local

# Model configuration by provider
openai:
  model: gpt-4
  temperature: 0.7
  max_tokens: 1000

anthropic:
  model: claude-3-5-sonnet-20241022
  temperature: 0.7
  max_tokens: 1000

# Set your API keys as environment variables:
# export OPENAI_API_KEY=sk-...
# export ANTHROPIC_API_KEY=sk-ant-...

# Cost management (estimated USD per 1M tokens)
cost_limits:
  max_daily_spend: 50.0
  warn_threshold: 40.0

# Decision caching - avoid re-analyzing identical situations
cache:
  enabled: true
  ttl_hours: 1.0
